`r read_chunk('tests/tutorial.R')`
`r opts_chunk$set(echo=TRUE, tidy=FALSE, comment="")`

```{r setup}
# the default output hook
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x = unlist(stringr::str_split(x, '\n'))
    if (length(x) > n) {
      # truncate the output
      x = c(head(x, n), '....\n')
    }
    x = paste(x, collapse = '\n') # paste first n lines together
  }
  hook_output(x, options)
})
opts_chunk$set(out.lines = 8)
```

# plyrmr


Load package and turn off Hadoop for quick demo
```{r}
suppressMessages(library(plyrmr))
rmr.options(backend = "local")
```

Create a dataset with `input`

```{r}
mtcars.in = input(mtcars)
```

or simply start from a hdfs path then select some larger engines

```{r}
mtcars.5cyl.up = subset(mtcars.in, cyl > 4)
```

then group them by cyl

```{r}
grouped = group.by(mtcars.5cyl.up, "cyl")
```

then look at the average carbs

```{r}
avg.carbs = summarize(grouped, mean(carb), mean.HP = mean(hp))
```

nothing happened yet, is it real?

```{r}
as.data.frame(avg.carbs)
```

This triggers a mapred job and brings the result into mem as a df. If it's too big, you can write it to a specific location.

```{r}
file.remove("/tmp/avg.carbs")
avg.carbs.out = output(avg.carbs, "/tmp/avg.carbs")
```

You can still read it, if small enough

```{r}
as.data.frame(avg.carbs.out)
```

Most functions are modeled after familiar ones

```
  transform #from base
  subset #from base
  mutate #from plyr
  summarize #from plyr
  select # synonym for summarize
```

`group.by` takes some dataset and column specs. `group.by.f` takes a function that generates the grouping columns on the fly.
`do` takes a data set and a function and appliess it to chunks of data. It's neither a map or a reduce, this is decided based on 
how it combines with `group.by`. All the functions listed above are implemented with `do` in one line of code. An actual MR job 
is triggered by `from.dfs`, `output` or combining two of `group.by` or `group.by.f` together, since we can't easily optimize
away two groupings into one reduce phase. Comments and suggestions to rhadoop@revolutionanalytics.com.


## Tutorial

To identify input data we need the function `input`. If we want to process file `"some/path"`, we need to call `input("some/path")`. If we want to create a small data set on the fly, we can pass a data frame as argument. This is most useful for learning and testing purposes. This is an example of the latter: 
```{r input}
```
Also for compatibility with `rmr2` we can  the output of a `mapreduce` call to `input`.
The reverse step is to take some data and turn it into a data frame (do this only on small data sets such as in this example):

```{r as.data.frame}
```

Let's start now with some simple processing, like taking the square of some numbers. In R and particularly using the `plyr` package and its approach to data manipulation, you could proceed as follows. First create a data frame with some numbers:
```{r small-integers}
```

Then add a column of squares with `mutate` (which is very similar to `transform` in the `base` package).

```{r squares-data-frame}
```

Let's make this an input data set according to the `plyrmr`.

```{r input-small-integers}
```

We can call `mutate` on this data set and store the result in a variable. It doesn't look like that variable has data at all in it, in fact it doesn't. It's a `pipe`, a description of a sequence of processing steps. Nothing gets actually computed until necessary. 

```{r squares-plyrmr}
```

But if we turn a `pipe` into a data frame, we see the data as expected. 

```{r squares-results}
```

Turning a `pipe` into a data frame is one of a few triggering events that will start the actual computation. This is powered by `rmr2`, hence it can be hadoop backed, hence it can operate on very large data sets. An almost identical syntax can be used to perform the same operation on a data frame and a Hadoop data set. When operating on very large data sets, we can't use `as.data.frame`, because there isn't enough RAM available. The alternative is the `output` primitive, which will trigger the actual computation described by a `pipe` and store the results to a user-specified path:

```{r output}
```

And let's check that it actually worked:
```{r  as.data.frame-output}
```
With `output` and refraining from using `as.data.frame` we can process hadoop sized data sets. Of course we can use `as.data.frame` after a number of data reduction steps.

Let's move to some counting task. We create a data frame with a single column containing a sample from the binomial distribution, just for illustration purposes.

```{r binomial-sample}
```

Counting the number of occurrences of each outcome is a single line task in `plyr`. `ddply` splits a data frame according to a variable and summarize creates a new data frame with the columns specified in its additional arguments.

```{r count-data-frame}
```

Let's create a `plyrmr` data set with `input`

```{r input-binomial-sample}
```

The equivalent in `plyrmr` is not as close in syntax as before, because we followed more closely the syntax of an experimental package by the same author as `plyr` called `dplyr`, which is focused on data frames and adds multiple backends and can be considered a specialization and evolution of `plyr`. `dplyr` is temporarily incompatible with `rmr2` and not as well known as `plyr` yet and so it is not used here, but was a reference point in the design of `plyrmr`. `plyrmr`, like `dplyr` has a separate `group.by` primitive, named after its SQL equivalent, that defines a grouping of a data set based on a column (expressions are not supported yet).

```{r count-plyrmr}
```

What we can see here is that we can combine two `pipes` the same way we compose two functions. We can check the results with

```{r count-results}
```
Please not that the results are not in the same order. This is always true with Hadoop and if other examples in this tutorial seem to show the opposite it's only becuase of the tiny size of the data sets involved. Not incidentally, theoreticians have  formalized this computational model as MUD (Massive Unordered Distributed, see [this paper](http://arxiv.org/abs/cs/0611108)).

```{r identity-data-frame}
```

```{r identity-plyrmr}
```

```{r identity-do}
```

```{r subset-data-frame}
```

```{r subset-plyrmr}
```

```{r select-data-frame}
```

```{r select-plyrmr}
```

```{r select-plyrmr-alternative}
```

```{r big-sum-data-frame}
```

```{r big-sum-plyrmr}
```

```{r group-sum-data-frame}
```

```{r group-sum-plyrmr}
```

```{r textual-data}
```
 
```{r wordcount-data-frame}
```

```{r wordcount-plyrmr}
```



