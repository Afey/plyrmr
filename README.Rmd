`r read_chunk('tests/tutorial.R')`
`r opts_chunk$set(echo=TRUE, tidy=FALSE, comment="")`

```{r setup, echo=FALSE}
# the default output hook
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x = unlist(stringr::str_split(x, '\n'))
    if (length(x) > n) {
      # truncate the output
      x = c(head(x, n), '....\n')
    }
    x = paste(x, collapse = '\n') # paste first n lines together
  }
  hook_output(x, options)
})
opts_chunk$set(out.lines = 8)
```

# plyrmr

Load package and turn off Hadoop for quick demo
```{r}
suppressMessages(library(plyrmr))
rmr.options(backend = "local")
rmr.options(keyval.length = 5)
set.seed(0)
```

Create a dataset with `input`

```{r}
mtcars.in = input(mtcars)
```

or simply start from a HDFS path then select some larger engines

```{r}
mtcars.5cyl.up = subset(mtcars.in, cyl > 4)
```

then group them by cyl

```{r}
grouped = group.by(mtcars.5cyl.up, cyl)
```

then look at the average carbs

```{r}
avg.carbs = summarize(grouped, mean(carb), mean.HP = mean(hp))
```

nothing happened yet, is it real?

```{r}
as.data.frame(avg.carbs)
```

This triggers a mapreduce job and brings the result into memory as a data frame. If it's too big, you can write it to a specific location.

```{r}
file.remove("/tmp/avg.carbs")
avg.carbs.out = output(avg.carbs, "/tmp/avg.carbs")
```

You can still read it, if small enough

```{r}
as.data.frame(avg.carbs.out)
```

Most functions are modeled after familiar ones

```
  transform #from base
  subset #from base
  mutate #from plyr
  summarize #from plyr
  select # synonym for summarize
```

`group.by` takes some dataset and column specs. `group.by.f` takes a function that generates the grouping columns on the fly.
`do` takes a data set and a function and applies it to chunks of data. It's neither a map or a reduce, this is decided based on 
how it combines with `group.by`. All the functions listed above are implemented with `do` in one line of code. An actual MR job 
is triggered by `from.dfs`, `output` or combining two of `group.by` or `group.by.f` together, since we can't easily optimize
away two groupings into one reduce phase. Comments and suggestions to rhadoop@revolutionanalytics.com.


## Tutorial

To identify input data we need the function `input`. If we want to process file `"some/path"`, we need to call `input("some/path")`. If we want to create a small data set on the fly, we can pass a data frame as argument. This is most useful for learning and testing purposes. This is an example of the latter: 
```{r input}
```
Also for compatibility with `rmr2` we can pass the output of a `mapreduce` call to `input`.
The reverse step is to take some data and turn it into a data frame (do this only on small data sets such as in this example):

```{r as.data.frame}
```

Let's start now with some simple processing, like taking the square of some numbers. In R and particularly using the `plyr` package and its approach to data manipulation, you could proceed as follows. First create a data frame with some numbers:
```{r small-integers}
```

Then add a column of squares with `mutate` (which is very similar to `transform` in the `base` package).

```{r squares-data-frame}
```

Let's make this an input data set according to the `plyrmr`.

```{r input-small-integers}
```

We can call `mutate` on this data set and store the result in a variable. It doesn't look like that variable has data at all in it, in fact it doesn't. It's a `pipe`, a description of a sequence of processing steps. Nothing gets actually computed until necessary. 

```{r squares-plyrmr}
```

But if we turn a `pipe` into a data frame, we see the data as expected. 

```{r squares-results}
```

Turning a `pipe` into a data frame is one of a few triggering events that will start the actual computation. This is powered by `rmr2`, hence it can be hadoop backed, hence it can operate on very large data sets. An almost identical syntax can be used to perform the same operation on a data frame and a Hadoop data set. When operating on very large data sets, we can't use `as.data.frame`, because there isn't enough RAM available. The alternative is the `output` primitive, which will trigger the actual computation described by a `pipe` and store the results to a user-specified path:

```{r output}
```

And let's check that it actually worked:
```{r  as.data.frame-output}
```
With `output` and refraining from using `as.data.frame` we can process hadoop sized data sets. Of course we can use `as.data.frame` after a number of data reduction steps. Another role of output is as a bridge with `rmr2`. You can just write `mapreduce(ouput(...))` and combine the best of the two packages.

Let's move to some counting task. We create a data frame with a single column containing a sample from the binomial distribution, just for illustration purposes.

```{r binomial-sample}
```

Counting the number of occurrences of each outcome is a single line task in `plyr`. `ddply` splits a data frame according to a variable and summarize creates a new data frame with the columns specified in its additional arguments.

```{r count-data-frame}
```

Let's create a `plyrmr` data set with `input`

```{r input-binomial-sample}
```

The equivalent in `plyrmr` is not as close in syntax as before, because we followed more closely the syntax of an experimental package by the same author as `plyr` called `dplyr`, which is focused on data frames and adds multiple backends and can be considered a specialization and evolution of `plyr`. `dplyr` is temporarily incompatible with `rmr2` and not as well known as `plyr` yet and so it is not used here, but was a reference point in the design of `plyrmr`. `plyrmr`, like `dplyr` has a separate `group.by` primitive (`group_by` in `dplyr`), named after its SQL equivalent, that defines a grouping of a data set based on a column (expressions are not supported yet).

```{r count-plyrmr}
```

What we can see here is that we can combine two `pipes` by composing two functions. We can check the results with

```{r count-results}
```
Please note that the results are not in the same order. This is always true with Hadoop and if other examples in this tutorial seem to show the opposite it's only because of the tiny size of the data sets involved. Not incidentally, theoreticians have formalized this computational model as MUD (Massive Unordered Distributed, see [this paper](http://arxiv.org/abs/cs/0611108)). 

Writing an identity function is not particularly interesting and won't make you rich, but it's a boilerplate test. Here is one way of expressing the identity in R:

```{r identity-data-frame}
```

And here is the equivalent in plyrmr.

```{r identity-plyrmr}
```

Now let's take a baby step: select certain rows. The function `subset` in `base` comes in handy.

```{r subset-data-frame}
```

We now can do exactly the same on a Hadoop data set:

```{r subset-plyrmr}
```

Next baby step up from selecting rows is selecting columns:
```{r select-data-frame}
```

And in `plyrmr`
```{r select-plyrmr}
```

Deceptively similar, but works on petabytes. In fact `summarize` doesn't seem the right name for this function, which can do a lot more. So we aliased to `select`, following dplyr, to allow the programmer to express intent.

```{r select-plyrmr-alternative}
```

We are now going to tackle the extreme data reduction task, whereby we go from a data set to a single number (per column), in this case taking the sum. This is very simple in 	`plyr`

```{r big-sum-data-frame}
```

but a little more complex in `plyrmr`, and why that's the case merits a little explanation. `plyr::summarize` works on data frames and has all the data available simultaneously. This is not true for "plyrmr" because large data sets are processed piecemeal. So we need to perform the sum on each chunk of data, group the results together, sum again. `group.by(data, 1)` just means group everything together, in fact there is handy alias for that, `group.together` 

```{r big-sum-plyrmr}
```

In many other use cases, instead of a single summary, we are interested in summaries by group. In `plyr` this calls for the `ddply` function (`group_by` in `dplyr`)
```{r group-sum-data-frame}
```

The equivalent in `plyrmr` is `group.by`

```{r group-sum-plyrmr}
```

We are ready to write the wordcount function, the "hello world" equivalent of the Hadoop world. The task is to read in a data frame with lines of text, split the lines into words and count how many times each word occurs. First let's make up some fake text data to keep things self-contained.
```{r textual-data}
```
This is how the task can be accomplised working on a data frame. 
 
```{r wordcount-data-frame}
```

In fact the name `summarize` seems again unsatisfactory for self-documenting code here. We are looking for at least an alias that could capture this kind of usage. Maybe `explode`? 

```{r wordcount-plyrmr}
```

## The fundamental primitives: `do`  and `group.by.f`



