`r read_chunk('../tests/tutorial.R')`
`r opts_chunk$set(echo=TRUE, tidy=FALSE, comment="")`

```{r setup, echo=FALSE}
# the default output hook
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x = unlist(stringr::str_split(x, '\n'))
    if (length(x) > n) {
      # truncate the output
      x = c(head(x, n), '....\n')
    }
    x = paste(x, collapse = '\n') # paste first n lines together
  }
  hook_output(x, options)
})
opts_chunk$set(out.lines = 8)
```

# Tutorial

## Introduction

`Plyrmr` is a package that allows to manipulate very large data sets (VLDSs). Because it's backed by Hadoop, large means up to petabytes in scale. It's based on another package, `rmr2`, which provides the foundation for writing programs according to the mapreduce paradigm. `Plyrmr` endeavors to add a layer of abstraction and, hopefully, conveninence and ease of use on top of `rmr2`. To achieve that, it takes inspiration from the data manipulation style promoted by Hadley Wickham in his very popular packages `plyr` and `reshape2` and the very new `dplyr` and adapts it to the VLDS setting.

## Basic concepts


### Programming model

VLDSs reside on disk and on multiple machines and they need to be processed on groups of machines called clusters, by definition of *very large*. Therefore, you can't load them as a whole into R, that is into main memory at any given time. You can only process small chunks, regroup them, process the resulting chunks and repeat. This matches very nicely into Hadley's organization of basic data manipulations into a system called "split-apply-combine", with one fundamental difference: the combine and split operation are combined into a single operation. This is necessary, because VLDSs are too big to exist in their "combined" form: they can exist only in one of many "split" states: fortunately we have control on how the split is defined. So instead of Hadley's three pronged approach to data manipulation, we have a two pronged one that maps naturally to the underlying map-reduce paradigm. The idea of `plyrmr` is to take the `plyr` style of data processing and map it to the underlying mapreduce paradigm trying to keep a similar syntax and semantics.   

### Data model

The model is the data.frame. The data is always represented as data frames, each of which contains part of the data. People used to `rmr2` or `plyr` may find this a restriction, and indeed it was a simplifying assumption in the desing phase. 

### Initialization

```{r load-library}
```

```{r local-backend}
```


### Building blocks

In `plyrmr` there are `pipes`, which are R expressions describing a computation. The simplest pipe has an input. The input can be a data frame, a file or another pipe, the first two of which need to be wrapped in an `input` call. The computation can be triggered with either `output` or `as.data.frame`, the first resulting in the data being written to a file and the second creating a data frame. So a trivial pipe could be

```
as.data.frame(input(mtcars))
```

To do something useful we need to create more complex expressions combining essentially two sets of basic ingredients: processing steps and grouping steps. We can also combine pipes into more complex ones. As we said, the data is always chunked, due to its size. The initial state is arbitrary, meaning that the data is sorted and grouped arbitrarily, but grouping can be modified with the primitive `group.f`. The fundamental processing primitive is `do`, which applies a function to each chunk. The function should transform a data frame into another data frame. For instance, if we wanted to compute the number of carburators per cylinder for the cars in the `mtcars` data set (all of 32, of course this is only for illustrative purposes), one could simply do

```
transform(mtcars, ratio = carb/cyl)

```

This is a regular data frame processing function from the package `base`. Now what if we wanted to do this on a VLDS? This is a row-by-row operation, hence the order and grouping of data does not matter. This allows us to define a pipe as follows

```
do(input(mtcars), transform, ratio = carb/cyl)
```













To identify input data we need the function `input`. If we want to process file `"some/path"`, we need to call `input("some/path")`. If we want to create a small data set on the fly, we can pass a data frame as argument. This is most useful for learning and testing purposes. This is an example of the latter: 

```{r input}
```
Also for compatibility with `rmr2` we can pass the output of a `mapreduce` call to `input`.
The reverse step is to take some data and turn it into a data frame (do this only on small data sets such as in this example):
	
```{r as.data.frame}
```

Let's start now with some simple processing, like taking the square of some numbers. In R and particularly using the `plyr` package and its approach to data manipulation, you could proceed as follows. First create a data frame with some numbers:
```{r small-integers}
```

Then add a column of squares with `mutate` (which is very similar to `transform` in the `base` package).

```{r squares-data-frame}
```

Let's make this an input data set according to the `plyrmr`.

```{r input-small-integers}
```

We can call `mutate` on this data set and store the result in a variable. It doesn't look like that variable has data at all in it, in fact it doesn't. It's a `pipe`, a description of a sequence of processing steps. Nothing gets actually computed until necessary. 

```{r squares-plyrmr}
```

But if we turn a `pipe` into a data frame, we see the data as expected. 

```{r squares-results}
```

Turning a `pipe` into a data frame is one of a few triggering events that will start the actual computation. This is powered by `rmr2`, hence it can be hadoop backed, hence it can operate on VLDSs. An almost identical syntax can be used to perform the same operation on a data frame and a Hadoop data set. When operating on VLDSs, we can't use `as.data.frame`, because there isn't enough RAM available. The alternative is the `output` primitive, which will trigger the actual computation described by a `pipe` and store the results to a user-specified path:

```{r output}
```

And let's check that it actually worked:
	```{r  as.data.frame-output}
```
With `output` and refraining from using `as.data.frame` we can process hadoop sized data sets. Of course we can use `as.data.frame` after a number of data reduction steps. Another role of output is as a bridge with `rmr2`. You can just write `mapreduce(ouput(...))` and combine the best of the two packages.

Let's move to some counting task. We create a data frame with a single column containing a sample from the binomial distribution, just for illustration purposes.

```{r binomial-sample}
```

Counting the number of occurrences of each outcome is a single line task in `plyr`. `ddply` splits a data frame according to a variable and summarize creates a new data frame with the columns specified in its additional arguments.

```{r count-data-frame}
```

Let's create a `plyrmr` data set with `input`

```{r input-binomial-sample}
```

The equivalent in `plyrmr` is not as close in syntax as before, because we followed more closely the syntax of an experimental package by the same author as `plyr` called `dplyr`, which is focused on data frames and adds multiple backends and can be considered a specialization and evolution of `plyr`. `dplyr` is temporarily incompatible with `rmr2` and not as well known as `plyr` yet and so it is not used here, but was a reference point in the design of `plyrmr`. `plyrmr`, like `dplyr` has a separate `group` primitive (`group_by` in `dplyr`), named after its SQL equivalent, that defines a grouping of a data set based on a column (expressions are not supported yet).

```{r count-plyrmr}
```

What we can see here is that we can combine two `pipes` by composing two functions. We can check the results with

```{r count-results}
```
Please note that the results are not in the same order. This is always true with Hadoop and if other examples in this tutorial seem to show the opposite it's only because of the tiny size of the data sets involved. Not incidentally, theoreticians have formalized this computational model as MUD (Massive Unordered Distributed, see [this paper](http://arxiv.org/abs/cs/0611108)). 

Writing an identity function is not particularly interesting and won't make you rich, but it's a boilerplate test. Here is one way of expressing the identity in R:

```{r identity-data-frame}
```

And here is the equivalent in plyrmr.

```{r identity-plyrmr}
```

Now let's take a baby step: select certain rows. The function `subset` in `base` comes in handy.

```{r subset-data-frame}
```

We now can do exactly the same on a Hadoop data set:
	
	```{r subset-plyrmr}
```

Next baby step up from selecting rows is selecting columns:
	```{r select-data-frame}
```

And in `plyrmr`
```{r select-plyrmr}
```

Deceptively similar, but works on petabytes. In fact `summarize` doesn't seem the right name for this function, which can do a lot more. So we aliased to `select`, following dplyr, to allow the programmer to express intent.

```{r select-plyrmr-alternative}
```

We are now going to tackle the extreme data reduction task, whereby we go from a data set to a single number (per column), in this case taking the sum. This is very simple in 	`plyr`

```{r big-sum-data-frame}
```

but a little more complex in `plyrmr`, and why that's the case merits a little explanation. `plyr::summarize` works on data frames and has all the data available simultaneously. This is not true for "plyrmr" because large data sets are processed piecemeal. So we need to perform the sum on each chunk of data, group the results together, sum again. `group(data, 1)` just means group everything together, in fact there is handy alias for that, `group.together` 

```{r big-sum-plyrmr}
```

In many other use cases, instead of a single summary, we are interested in summaries by group. In `plyr` this calls for the `ddply` function (`group_by` in `dplyr`)
	```{r group-sum-data-frame}
```

The equivalent in `plyrmr` is `group`

```{r group-sum-plyrmr}
```

We are ready to write the wordcount function, the "hello world" equivalent of the Hadoop world. The task is to read in a data frame with lines of text, split the lines into words and count how many times each word occurs. First let's make up some fake text data to keep things self-contained.

```{r textual-data}
```

This is how the task can be accomplised working on a data frame. 
 

```{r wordcount-data-frame}
```

In fact the name `summarize` seems again unsatisfactory for self-documenting code here. We are looking for at least an alias that could capture this kind of usage. Maybe `explode`? 

```{r wordcount-plyrmr}
```

## The fundamental primitives: `do`  and `group.f`




